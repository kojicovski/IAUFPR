{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "exercise_2_sentiment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIofW8wlQVnj"
      },
      "source": [
        "Curso de Especialização de Inteligência Artificial Aplicada\n",
        "\n",
        "Setor de Educação Profissional e Tecnológica - SEPT\n",
        "\n",
        "Universidade Federal do Paraná - UFPR\n",
        "\n",
        "---\n",
        "\n",
        "**IAA003 - Linguagem de Programação Aplicada**\n",
        "\n",
        "Prof. Alexander Robert Kutzke\n",
        "\n",
        "# Implementação com Scikit-Learn\n",
        "\n",
        "Utilizando a base de dados presente no repositório:\n",
        "\n",
        "1. Escreva *pipeline de classificação de texto* para classificar reviews de filmes como positivos e negativos;\n",
        "2. Encontre um bom conjunto de parâmetros utilizando `GridSearchCV`;\n",
        "3. Avalie o classificador utilizando parte do conjunto de dados (previamente separado para testes).\n",
        "4. Repita os passos 1, 2 e 3 utilizando um algoritmo de classificação diferente;\n",
        "5. Escreva um pequeno texto comparando os resultados obtidos para cada algoritmo.\n",
        "\n",
        "O texto pode ser escrito em um \"Jupyter Notebook\" juntamente com o código. Ou qualquer outro tipo de documento.\n",
        "\n",
        "--------------------------------------------------------------------------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGl5hRAoRiMF"
      },
      "source": [
        "Realizando imports das bibliotecas utilizadas para desenvolvimento e respostas dos exercícios propostos pelo prof. Alex."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofcJPX1XQ8WM"
      },
      "source": [
        "\"\"\"Build a sentiment analysis / polarity model\n",
        "\n",
        "Sentiment analysis can be casted as a binary text classification problem,\n",
        "that is fitting a linear classifier on features extracted from the text\n",
        "of the user messages so as to guess wether the opinion of the author is\n",
        "positive or negative.\n",
        "\n",
        "In this examples we will use a movie review dataset.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.datasets import load_files\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn import metrics\n",
        "import numpy as np\n",
        "from datetime import datetime"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "olYwBo5TV3AK",
        "outputId": "e41deba2-951b-4c90-911f-3d972e906d6a"
      },
      "source": [
        "# Choose a clafissier, SDGClassifier [SDG] or Multinomial.\n",
        "classifier = 'SGD'\n",
        "mean_pipe = []\n",
        "mean_SV = []\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # NOTE: we put the following in a 'if __name__ == \"__main__\"' protected\n",
        "    # block to be able to use a multi-core grid search that also works under\n",
        "    # Windows, see:http://docs.python.org/library/multiprocessing.html#windows\n",
        "    # The multiprocessing module is used as the backend of joblib.Parallel\n",
        "    # that is used when n_jobs != 1 in GridSearchCV\n",
        "    \n",
        "    start = datetime.now()\n",
        "    # the training data folder must be passed as first argument\n",
        "    movie_reviews_data_folder = r\"/home/Documents/sentiment_exercise/data\"\n",
        "    dataset = load_files(movie_reviews_data_folder, shuffle=False)\n",
        "    print(\"n_samples: %d\" % len(dataset.data))\n",
        "    \n",
        "\n",
        "    # split the dataset in training and test set:\n",
        "    docs_train, docs_test, y_train, y_test = train_test_split(\n",
        "        dataset.data, dataset.target, test_size=0.25, random_state=None)\n",
        "    \n",
        "    # TASK: Build a vectorizer / classifier pipeline that filters out tokens\n",
        "    if classifier == 'Multinomial':\n",
        "      text_clf = Pipeline([('vect', TfidfVectorizer()),\n",
        "                        ('tfidf', TfidfTransformer()),\n",
        "                        ('clf', MultinomialNB()),\n",
        "      ])\n",
        "    elif classifier == 'SDG':\n",
        "      text_clf = Pipeline([('vect', TfidfVectorizer()),\n",
        "                        ('tfidf', TfidfTransformer()),\n",
        "                        ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
        "                                              alpha=1e-3, random_state=60,\n",
        "                                              max_iter=5, tol=None)),\n",
        "                        ])\n",
        "\n",
        "    text_clf.fit(docs_train, y_train)   \n",
        "    predicted = text_clf.predict(docs_test)\n",
        "\n",
        "    print(\"###############Usando apenas o Pipeline###############\")\n",
        "    print(metrics.confusion_matrix(y_test, predicted))\n",
        "    print(text_clf, metrics.classification_report(y_test, predicted))\n",
        "    print(np.mean(predicted == y_test))\n",
        "    \n",
        "    \n",
        "    # TASK: Build a grid search to find out whether unigrams or bigrams are\n",
        "    # more useful.\n",
        "    # Fit the pipeline on the training set using grid search for the \n",
        "    # parameters\n",
        "    \n",
        "    parameters = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
        "                  'tfidf__use_idf': (True, False),\n",
        "                  'clf__alpha': (1e-2, 1e-3),\n",
        "                  }\n",
        "    \n",
        "    gs_clf = GridSearchCV(text_clf, parameters, n_jobs=-1)\n",
        "    gs_clf = gs_clf.fit(docs_train, y_train)\n",
        "       \n",
        "\n",
        "    # TASK: print the cross-validated scores for the each parameters set\n",
        "    # explored by the grid search\n",
        "    for param_name in sorted(parameters.keys()):\n",
        "        print(\"%s: %r\" % (param_name, gs_clf.best_params_[param_name]))\n",
        "\n",
        "    # TASK: Predict the outcome on the testing set and store it in a variable\n",
        "    # named y_predicted\n",
        "    \n",
        "    y_predicted = gs_clf.predict(docs_test)\n",
        "    \n",
        "    cm = metrics.confusion_matrix(y_test, y_predicted)\n",
        "    print(\"\\n ###############GridSearchCV###############\")\n",
        "    print(cm)\n",
        "    print(gs_clf, metrics.classification_report(y_test, y_predicted))\n",
        "    print(np.mean(y_predicted == y_test))\n",
        "    \n",
        "    end = datetime.now()\n",
        "    tmp = end - start\n",
        "    \n",
        "    print(tmp)\n",
        "\n",
        "    # import matplotlib.pyplot as plt\n",
        "    # plt.matshow(cm)\n",
        "    # plt.show()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "n_samples: 2000\n",
            "###############Usando apenas o Pipeline###############\n",
            "[[198  41]\n",
            " [ 58 203]]\n",
            "Pipeline(memory=None,\n",
            "         steps=[('vect',\n",
            "                 TfidfVectorizer(analyzer='word', binary=False,\n",
            "                                 decode_error='strict',\n",
            "                                 dtype=<class 'numpy.float64'>,\n",
            "                                 encoding='utf-8', input='content',\n",
            "                                 lowercase=True, max_df=1.0, max_features=None,\n",
            "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
            "                                 preprocessor=None, smooth_idf=True,\n",
            "                                 stop_words=None, strip_accents=None,\n",
            "                                 sublinear_tf=False,\n",
            "                                 token_pattern='(...\n",
            "                ('clf',\n",
            "                 SGDClassifier(alpha=0.001, average=False, class_weight=None,\n",
            "                               early_stopping=False, epsilon=0.1, eta0=0.0,\n",
            "                               fit_intercept=True, l1_ratio=0.15,\n",
            "                               learning_rate='optimal', loss='hinge',\n",
            "                               max_iter=5, n_iter_no_change=5, n_jobs=None,\n",
            "                               penalty='l2', power_t=0.5, random_state=60,\n",
            "                               shuffle=True, tol=None, validation_fraction=0.1,\n",
            "                               verbose=0, warm_start=False))],\n",
            "         verbose=False)               precision    recall  f1-score   support\n",
            "\n",
            "           1       0.77      0.83      0.80       239\n",
            "           2       0.83      0.78      0.80       261\n",
            "\n",
            "    accuracy                           0.80       500\n",
            "   macro avg       0.80      0.80      0.80       500\n",
            "weighted avg       0.80      0.80      0.80       500\n",
            "\n",
            "0.802\n",
            "clf__alpha: 0.001\n",
            "tfidf__use_idf: False\n",
            "vect__ngram_range: (1, 1)\n",
            "\n",
            " ###############GridSearchCV###############\n",
            "[[213  26]\n",
            " [ 75 186]]\n",
            "GridSearchCV(cv=None, error_score=nan,\n",
            "             estimator=Pipeline(memory=None,\n",
            "                                steps=[('vect',\n",
            "                                        TfidfVectorizer(analyzer='word',\n",
            "                                                        binary=False,\n",
            "                                                        decode_error='strict',\n",
            "                                                        dtype=<class 'numpy.float64'>,\n",
            "                                                        encoding='utf-8',\n",
            "                                                        input='content',\n",
            "                                                        lowercase=True,\n",
            "                                                        max_df=1.0,\n",
            "                                                        max_features=None,\n",
            "                                                        min_df=1,\n",
            "                                                        ngram_range=(1, 1),\n",
            "                                                        norm='l2',\n",
            "                                                        preprocessor=None,\n",
            "                                                        smooth_idf=True,\n",
            "                                                        stop_words=None,\n",
            "                                                        strip_a...\n",
            "                                                      n_jobs=None, penalty='l2',\n",
            "                                                      power_t=0.5,\n",
            "                                                      random_state=60,\n",
            "                                                      shuffle=True, tol=None,\n",
            "                                                      validation_fraction=0.1,\n",
            "                                                      verbose=0,\n",
            "                                                      warm_start=False))],\n",
            "                                verbose=False),\n",
            "             iid='deprecated', n_jobs=-1,\n",
            "             param_grid={'clf__alpha': (0.01, 0.001),\n",
            "                         'tfidf__use_idf': (True, False),\n",
            "                         'vect__ngram_range': [(1, 1), (1, 2)]},\n",
            "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
            "             scoring=None, verbose=0)               precision    recall  f1-score   support\n",
            "\n",
            "           1       0.74      0.89      0.81       239\n",
            "           2       0.88      0.71      0.79       261\n",
            "\n",
            "    accuracy                           0.80       500\n",
            "   macro avg       0.81      0.80      0.80       500\n",
            "weighted avg       0.81      0.80      0.80       500\n",
            "\n",
            "0.798\n",
            "0:01:11.442339\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5YakZHZYmwE"
      },
      "source": [
        "------------------------------------------------------------------\n",
        "Através do classificador Multinomial Naive Bayes, modelo que utiliza a probababilidade de cada evento ocorrer, desconsiderando a correlação entre features obteve-se uma média de acurácia de precisão de 80% e um recall de 80%,utilizando apenas a função Pipeline. Já encontrando os melhores parâmetros através do GridSeachCV, conseguiu-se uma acurácia de 81%. Sendo os parâmetros:\n",
        "\n",
        "* clf__alpha: 0.01\n",
        "* tfidf__use_idf: False\n",
        "* vect__ngram_range: (1, 2)\n",
        "\n",
        "Verificando sua matriz de confusão é possível verificar que sua precisão é\n",
        "baixa:\n",
        "\n",
        "[[203  51]\n",
        "\n",
        "[ 53 193]]\n",
        "\n",
        "Seu tempo de execução (treinamento e teste) foi em média 1 min e 25 segundos.\n",
        "\n",
        "Utilizando o classificador SGDClassifier, obteve-se um aumento de acurácia pouco significativa, de 82% utilizando apenas a função de Pipeline(). Já utilizando a fução de GridSearchCV(), obteve-se uma precisão de 83%. Os parâmetros melhores encontrados foram:\n",
        "\n",
        "\n",
        "* clf__alpha: 0.001\n",
        "* tfidf__use_idf: False\n",
        "* vect__ngram_range: (1, 1)\n",
        "\n",
        "Sua matriz de confusão:\n",
        "\n",
        "[[206  40]\n",
        "\n",
        "[ 45 209]]\n",
        "\n",
        "Seu tempo de execução com os parâmetros foi em média de 2 minutos.\n",
        "\n",
        "Observou-se que, embora o classificador SDG tenha conseguido uma acurácia de\n",
        "valor pouco maior que o classificador Multinomail Naive Bayer, seu tempo de \n",
        "execução foi maior. Para o tamanho do dataset utilizado, essa diferença leva\n",
        "a utilização do classificador SDG para identificar dos reviews de filmes."
      ]
    }
  ]
}